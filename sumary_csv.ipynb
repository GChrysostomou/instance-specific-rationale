{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": null,
>>>>>>> 9b21b56250ec89375e33333d93853b8da641b433
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with '/bin/python3' requires the jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pickle import NONE\n",
    "from re import T\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os \n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": null,
>>>>>>> 9b21b56250ec89375e33333d93853b8da641b433
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-8yj67l0k/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open('results_summary.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "ChnSentiCorp\n",
      "mbert\n",
      "{'Attention_Suff': 1.0902327957363036, 'Scaled_Attention_Suff': 1.0884435049783747, 'Gradients_Suff': 1.0483311770328323, 'Integrated_Gradients_Suff': 1.1026349831942344, 'Deeplift_Suff': 1.025998869133796, 'Attention_Comp': 1.3710924885883555, 'Scaled_Attention_Comp': 1.3789226438370283, 'Gradients_Comp': 1.156845532531631, 'Integrated_Gradients_Comp': 1.3241123432705728, 'Deeplift_Comp': 1.1598957879910066, 'Accuracy': 0.8577777777777771}\n",
      "zhbert\n",
      "{'Attention_Suff': 1.1186359490734263, 'Scaled_Attention_Suff': 1.1236845435293923, 'Gradients_Suff': 0.9976570862899498, 'Integrated_Gradients_Suff': 1.0772416350726395, 'Deeplift_Suff': 0.9511550052472773, 'Attention_Comp': 1.1876611990674635, 'Scaled_Attention_Comp': 1.1801661231830363, 'Gradients_Comp': 1.0394625200358314, 'Integrated_Gradients_Comp': 1.2453264958711667, 'Deeplift_Comp': 1.0270209744638243, 'Accuracy': 0.901944444444444}\n",
      "macbert\n",
      "{'Attention_Suff': 1.1122814556447151, 'Scaled_Attention_Suff': 1.1125499423523266, 'Gradients_Suff': 0.9963475341942042, 'Integrated_Gradients_Suff': 1.0640569513428195, 'Deeplift_Suff': 0.9854040891642601, 'Attention_Comp': 1.2308610402184026, 'Scaled_Attention_Comp': 1.2283784141257583, 'Gradients_Comp': 1.0676890567164459, 'Integrated_Gradients_Comp': 1.177746799422861, 'Deeplift_Comp': 1.024743013945636, 'Accuracy': 0.9119444444444442}\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 1.0362670022623193, 'Scaled_Attention_Suff': 1.0438528415303818, 'Gradients_Suff': 1.0352134211172692, 'Integrated_Gradients_Suff': 1.0105025424671388, 'Deeplift_Suff': 1.0231240681495706, 'Attention_Comp': 0.835423863402084, 'Scaled_Attention_Comp': 0.832508209594846, 'Gradients_Comp': 1.0052723427963992, 'Integrated_Gradients_Comp': 1.0383108812729012, 'Deeplift_Comp': 0.9857282003752925, 'Accuracy': 0.904999999999999}\n",
      "chinese_roberta\n",
      "{'Attention_Suff': 1.2077615094158674, 'Scaled_Attention_Suff': 1.197201305433269, 'Gradients_Suff': 1.0852296321172212, 'Integrated_Gradients_Suff': 1.2405948474855697, 'Deeplift_Suff': 1.094614465259573, 'Attention_Comp': 1.0775000545100044, 'Scaled_Attention_Comp': 1.0830489984189435, 'Gradients_Comp': 1.1097883259072419, 'Integrated_Gradients_Comp': 1.3698156574893492, 'Deeplift_Comp': 1.1278408373195472, 'Accuracy': 0.908333333333333}\n",
      "                 Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "mbert                  1.090233               1.088444        1.048331   \n",
      "zhbert                 1.118636               1.123685        0.997657   \n",
      "macbert                1.112281               1.112550        0.996348   \n",
      "xlm_roberta            1.036267               1.043853        1.035213   \n",
      "chinese_roberta        1.207762               1.197201        1.085230   \n",
      "\n",
      "                 Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "mbert                             1.102635       1.025999        1.371092   \n",
      "zhbert                            1.077242       0.951155        1.187661   \n",
      "macbert                           1.064057       0.985404        1.230861   \n",
      "xlm_roberta                       1.010503       1.023124        0.835424   \n",
      "chinese_roberta                   1.240595       1.094614        1.077500   \n",
      "\n",
      "                 Scaled_Attention_Comp  Gradients_Comp  \\\n",
      "mbert                         1.378923        1.156846   \n",
      "zhbert                        1.180166        1.039463   \n",
      "macbert                       1.228378        1.067689   \n",
      "xlm_roberta                   0.832508        1.005272   \n",
      "chinese_roberta               1.083049        1.109788   \n",
      "\n",
      "                 Integrated_Gradients_Comp  Deeplift_Comp  Accuracy  \\\n",
      "mbert                             1.324112       1.159896  0.857778   \n",
      "zhbert                            1.245326       1.027021  0.901944   \n",
      "macbert                           1.177747       1.024743  0.911944   \n",
      "xlm_roberta                       1.038311       0.985728  0.905000   \n",
      "chinese_roberta                   1.369816       1.127841  0.908333   \n",
      "\n",
      "                           model       dataset  \n",
      "mbert                      mbert  ChnSentiCorp  \n",
      "zhbert                    zhbert  ChnSentiCorp  \n",
      "macbert                  macbert  ChnSentiCorp  \n",
      "xlm_roberta          xlm_roberta  ChnSentiCorp  \n",
      "chinese_roberta  chinese_roberta  ChnSentiCorp  \n",
      " \n",
      "multirc\n",
      "mbert\n",
      "{'Attention_Suff': 1.1528163924002583, 'Scaled_Attention_Suff': 1.160154956255558, 'Gradients_Suff': 1.0001782820153244, 'Integrated_Gradients_Suff': 1.0782598688552258, 'Deeplift_Suff': 0.9993364021713032, 'Attention_Comp': 0.9955666407107236, 'Scaled_Attention_Comp': 0.992904356248007, 'Gradients_Comp': 1.0018762830182584, 'Integrated_Gradients_Comp': 1.0047978625144316, 'Deeplift_Comp': 1.0020655492592467, 'F1': 0.708081756873426}\n",
      "bert\n",
      "{'Attention_Suff': 1.295512460541634, 'Scaled_Attention_Suff': 1.2872221467110898, 'Gradients_Suff': 1.0363704378054417, 'Integrated_Gradients_Suff': 1.1011344793996825, 'Deeplift_Suff': 0.9856377618396626, 'Attention_Comp': 1.0066199174970396, 'Scaled_Attention_Comp': 1.009889976081217, 'Gradients_Comp': 0.9991115138577739, 'Integrated_Gradients_Comp': 0.9994490412652345, 'Deeplift_Comp': 1.0004289512679125, 'F1': 0.680454485654535}\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 1.0046606400004923, 'Scaled_Attention_Suff': 0.990761161318685, 'Gradients_Suff': 1.0753101427707308, 'Integrated_Gradients_Suff': 0.9583695279825207, 'Deeplift_Suff': 0.9846725254007059, 'Attention_Comp': 0.9736171964765234, 'Scaled_Attention_Comp': 0.9812189513478892, 'Gradients_Comp': 1.04316073157631, 'Integrated_Gradients_Comp': 1.064054814866461, 'Deeplift_Comp': 1.0118789017000591, 'F1': 0.580088084517109}\n",
      "roberta\n",
      "{'Attention_Suff': 1.116932509349061, 'Scaled_Attention_Suff': 1.1306242668182926, 'Gradients_Suff': 1.022002424171818, 'Integrated_Gradients_Suff': 1.0263058609200078, 'Deeplift_Suff': 1.1512188775914287, 'Attention_Comp': 1.1042383239597067, 'Scaled_Attention_Comp': 1.0958994463332727, 'Gradients_Comp': 1.1126248925720392, 'Integrated_Gradients_Comp': 1.074342383068467, 'Deeplift_Comp': 1.0546891921536654, 'F1': 0.72947136804884}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "mbert              1.152816               1.160155        1.000178   \n",
      "bert               1.295512               1.287222        1.036370   \n",
      "xlm_roberta        1.004661               0.990761        1.075310   \n",
      "roberta            1.116933               1.130624        1.022002   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "mbert                         1.078260       0.999336        0.995567   \n",
      "bert                          1.101134       0.985638        1.006620   \n",
      "xlm_roberta                   0.958370       0.984673        0.973617   \n",
      "roberta                       1.026306       1.151219        1.104238   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "mbert                     0.992904        1.001876                   1.004798   \n",
      "bert                      1.009890        0.999112                   0.999449   \n",
      "xlm_roberta               0.981219        1.043161                   1.064055   \n",
      "roberta                   1.095899        1.112625                   1.074342   \n",
      "\n",
      "             Deeplift_Comp        F1        model  dataset  \n",
      "mbert             1.002066  0.708082        mbert  multirc  \n",
      "bert              1.000429  0.680454         bert  multirc  \n",
      "xlm_roberta       1.011879  0.580088  xlm_roberta  multirc  \n",
      "roberta           1.054689  0.729471      roberta  multirc  \n",
      " \n",
      "ant\n",
      "mbert\n",
      "{'Attention_Suff': 1.0477384467759994, 'Scaled_Attention_Suff': 1.0449974576786865, 'Gradients_Suff': 0.9872483912617476, 'Integrated_Gradients_Suff': 1.0048719339127243, 'Deeplift_Suff': 0.9655902202994344, 'Attention_Comp': 1.1037677225925988, 'Scaled_Attention_Comp': 1.113315858846973, 'Gradients_Comp': 0.9932145224193266, 'Integrated_Gradients_Comp': 0.9806456166416007, 'Deeplift_Comp': 1.0196784787484527, 'Accuracy': 0.912716912716912}\n",
      "zhbert\n",
      "{'Attention_Suff': 1.0243519983478813, 'Scaled_Attention_Suff': 1.038452472139083, 'Gradients_Suff': 0.9815883008520067, 'Integrated_Gradients_Suff': 0.974429650586753, 'Deeplift_Suff': 0.9984249355334712, 'Attention_Comp': 1.0389534272744658, 'Scaled_Attention_Comp': 1.0276156119427946, 'Gradients_Comp': 1.0074105650526328, 'Integrated_Gradients_Comp': 1.0217447124035852, 'Deeplift_Comp': 1.0195815720060808, 'Accuracy': 0.722610722610722}\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 1.0256264456101158, 'Scaled_Attention_Suff': 1.029176927807333, 'Gradients_Suff': 0.9993468988690781, 'Integrated_Gradients_Suff': 1.0160364193286382, 'Deeplift_Suff': 1.0199820396867019, 'Attention_Comp': 1.2422812914442778, 'Scaled_Attention_Comp': 1.2205275854774038, 'Gradients_Comp': 0.9479272497268252, 'Integrated_Gradients_Comp': 0.9539908377230696, 'Deeplift_Comp': 1.070670544711341, 'Accuracy': 0.789346456013122}\n",
      "macbert\n",
      "{'Attention_Suff': 1.0175806223063184, 'Scaled_Attention_Suff': 1.0016088094179378, 'Gradients_Suff': 0.9690520008062631, 'Integrated_Gradients_Suff': 0.99810002290872, 'Deeplift_Suff': 0.9898352077322914, 'Attention_Comp': 0.9962425422560994, 'Scaled_Attention_Comp': 1.0052767329300833, 'Gradients_Comp': 1.0263562461964222, 'Integrated_Gradients_Comp': 1.0284065774233286, 'Deeplift_Comp': 1.0306502822314718, 'Accuracy': 0.701372701372701}\n",
      "chinese_roberta\n",
      "{'Attention_Suff': 1.0427954085352225, 'Scaled_Attention_Suff': 1.047221175884262, 'Gradients_Suff': 0.9788272699751466, 'Integrated_Gradients_Suff': 1.010807379094021, 'Deeplift_Suff': 0.9633009674181421, 'Attention_Comp': 1.002860484530994, 'Scaled_Attention_Comp': 0.9983234027644761, 'Gradients_Comp': 1.0265005321405132, 'Integrated_Gradients_Comp': 1.035994395975409, 'Deeplift_Comp': 1.0555151814812596, 'Accuracy': 0.702063368730035}\n",
      "                 Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "mbert                  1.047738               1.044997        0.987248   \n",
      "zhbert                 1.024352               1.038452        0.981588   \n",
      "xlm_roberta            1.025626               1.029177        0.999347   \n",
      "macbert                1.017581               1.001609        0.969052   \n",
      "chinese_roberta        1.042795               1.047221        0.978827   \n",
      "\n",
      "                 Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "mbert                             1.004872       0.965590        1.103768   \n",
      "zhbert                            0.974430       0.998425        1.038953   \n",
      "xlm_roberta                       1.016036       1.019982        1.242281   \n",
      "macbert                           0.998100       0.989835        0.996243   \n",
      "chinese_roberta                   1.010807       0.963301        1.002860   \n",
      "\n",
      "                 Scaled_Attention_Comp  Gradients_Comp  \\\n",
      "mbert                         1.113316        0.993215   \n",
      "zhbert                        1.027616        1.007411   \n",
      "xlm_roberta                   1.220528        0.947927   \n",
      "macbert                       1.005277        1.026356   \n",
      "chinese_roberta               0.998323        1.026501   \n",
      "\n",
      "                 Integrated_Gradients_Comp  Deeplift_Comp  Accuracy  \\\n",
      "mbert                             0.980646       1.019678  0.912717   \n",
      "zhbert                            1.021745       1.019582  0.722611   \n",
      "xlm_roberta                       0.953991       1.070671  0.789346   \n",
      "macbert                           1.028407       1.030650  0.701373   \n",
      "chinese_roberta                   1.035994       1.055515  0.702063   \n",
      "\n",
      "                           model dataset  \n",
      "mbert                      mbert     ant  \n",
      "zhbert                    zhbert     ant  \n",
      "xlm_roberta          xlm_roberta     ant  \n",
      "macbert                  macbert     ant  \n",
      "chinese_roberta  chinese_roberta     ant  \n",
      " \n",
      "agnews\n",
      "mbert\n",
      "{'Attention_Suff': 1.3018478196445673, 'Scaled_Attention_Suff': 1.303969427059124, 'Gradients_Suff': 1.0151901245198574, 'Integrated_Gradients_Suff': 1.2035095073555637, 'Deeplift_Suff': 1.0409249387751582, 'Attention_Comp': 1.6327321649621618, 'Scaled_Attention_Comp': 1.6322149420626877, 'Gradients_Comp': 1.231301912420742, 'Integrated_Gradients_Comp': 1.5172697211566961, 'Deeplift_Comp': 1.1937422579936299, 'F1': 0.9303252066670791}\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 1.2480589757676146, 'Scaled_Attention_Suff': 1.25022294963998, 'Gradients_Suff': 1.0172407653052353, 'Integrated_Gradients_Suff': 1.150860698782849, 'Deeplift_Suff': 0.9884756726080427, 'Attention_Comp': 1.2583351634061208, 'Scaled_Attention_Comp': 1.2582583700865417, 'Gradients_Comp': 1.0738678786928562, 'Integrated_Gradients_Comp': 1.2243996413818392, 'Deeplift_Comp': 1.0300978068580273, 'F1': 0.92612414549953}\n",
      "bert\n",
      "{'Attention_Suff': 1.1723421223468278, 'Scaled_Attention_Suff': 1.1603846611800244, 'Gradients_Suff': 1.01024896510935, 'Integrated_Gradients_Suff': 1.0995371610437523, 'Deeplift_Suff': 0.9843022553132014, 'Attention_Comp': 1.4087749879211249, 'Scaled_Attention_Comp': 1.42263054991959, 'Gradients_Comp': 1.0701250045077157, 'Integrated_Gradients_Comp': 1.3126895088160355, 'Deeplift_Comp': 1.1224180757738436, 'F1': 0.9357196909357691}\n",
      "roberta\n",
      "{'Attention_Suff': 1.1575345753369244, 'Scaled_Attention_Suff': 1.162057975140077, 'Gradients_Suff': 0.9575957109824451, 'Integrated_Gradients_Suff': 1.0931925581604196, 'Deeplift_Suff': 1.045917144515223, 'Attention_Comp': 1.214778470724602, 'Scaled_Attention_Comp': 1.2144406676231316, 'Gradients_Comp': 1.097345665592133, 'Integrated_Gradients_Comp': 1.1947390959100885, 'Deeplift_Comp': 1.031400650577354, 'F1': 0.9346675204372652}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "mbert              1.301848               1.303969        1.015190   \n",
      "xlm_roberta        1.248059               1.250223        1.017241   \n",
      "bert               1.172342               1.160385        1.010249   \n",
      "roberta            1.157535               1.162058        0.957596   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "mbert                         1.203510       1.040925        1.632732   \n",
      "xlm_roberta                   1.150861       0.988476        1.258335   \n",
      "bert                          1.099537       0.984302        1.408775   \n",
      "roberta                       1.093193       1.045917        1.214778   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "mbert                     1.632215        1.231302                   1.517270   \n",
      "xlm_roberta               1.258258        1.073868                   1.224400   \n",
      "bert                      1.422631        1.070125                   1.312690   \n",
      "roberta                   1.214441        1.097346                   1.194739   \n",
      "\n",
      "             Deeplift_Comp        F1        model dataset  \n",
      "mbert             1.193742  0.930325        mbert  agnews  \n",
      "xlm_roberta       1.030098  0.926124  xlm_roberta  agnews  \n",
      "bert              1.122418  0.935720         bert  agnews  \n",
      "roberta           1.031401  0.934668      roberta  agnews  \n",
      " \n",
      "csl\n",
      "zhbert\n",
      "{'Attention_Suff': 1.2127844943573733, 'Scaled_Attention_Suff': 1.216695302106607, 'Gradients_Suff': 0.9405321911658495, 'Integrated_Gradients_Suff': 0.9936711844562186, 'Deeplift_Suff': 1.021235237336812, 'Attention_Comp': 0.9950494609991279, 'Scaled_Attention_Comp': 0.9947986854193601, 'Gradients_Comp': 0.999208884713569, 'Integrated_Gradients_Comp': 0.9996401197257241, 'Deeplift_Comp': 0.9991146393430018, 'Accuracy': 0.8352222222222221}\n",
      "mbert\n",
      "{'Attention_Suff': 1.054446263056177, 'Scaled_Attention_Suff': 1.0723903896719662, 'Gradients_Suff': 0.982886863655955, 'Integrated_Gradients_Suff': 1.010146793872494, 'Deeplift_Suff': 0.9536929149444753, 'Attention_Comp': 0.9959337384651924, 'Scaled_Attention_Comp': 0.9953596512077547, 'Gradients_Comp': 1.000964424711906, 'Integrated_Gradients_Comp': 1.0056031362908142, 'Deeplift_Comp': 1.0089837155697354, 'Accuracy': 0.836111111111111}\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 1.059302194731507, 'Scaled_Attention_Suff': 1.055905340667893, 'Gradients_Suff': 0.9838518623731017, 'Integrated_Gradients_Suff': 1.0134826102762329, 'Deeplift_Suff': 0.989353220055808, 'Attention_Comp': 0.9636075585500276, 'Scaled_Attention_Comp': 0.9642875932746146, 'Gradients_Comp': 1.0152784617747084, 'Integrated_Gradients_Comp': 1.006101496170587, 'Deeplift_Comp': 1.0116419651380832, 'Accuracy': 0.7267777777777771}\n",
      "macbert\n",
      "{'Attention_Suff': 1.0929055292192846, 'Scaled_Attention_Suff': 1.0981508874019306, 'Gradients_Suff': 0.9977678839397409, 'Integrated_Gradients_Suff': 1.066133291876735, 'Deeplift_Suff': 1.0079961991770912, 'Attention_Comp': 1.0078509571048957, 'Scaled_Attention_Comp': 1.0073206340396437, 'Gradients_Comp': 1.002373427131691, 'Integrated_Gradients_Comp': 0.9970637147814939, 'Deeplift_Comp': 0.9959056289061775, 'Accuracy': 0.8464444444444441}\n",
      "chinese_roberta\n",
      "{'Attention_Suff': 1.054527656776905, 'Scaled_Attention_Suff': 1.0744670655830437, 'Gradients_Suff': 1.0089544077169796, 'Integrated_Gradients_Suff': 1.003920274943452, 'Deeplift_Suff': 1.0680116394186954, 'Attention_Comp': 1.0058642155760764, 'Scaled_Attention_Comp': 1.0051921594254056, 'Gradients_Comp': 0.9943873783457299, 'Integrated_Gradients_Comp': 0.9960380144050649, 'Deeplift_Comp': 1.0030613141907248, 'Accuracy': 0.851777777777777}\n",
      "                 Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "zhbert                 1.212784               1.216695        0.940532   \n",
      "mbert                  1.054446               1.072390        0.982887   \n",
      "xlm_roberta            1.059302               1.055905        0.983852   \n",
      "macbert                1.092906               1.098151        0.997768   \n",
      "chinese_roberta        1.054528               1.074467        1.008954   \n",
      "\n",
      "                 Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "zhbert                            0.993671       1.021235        0.995049   \n",
      "mbert                             1.010147       0.953693        0.995934   \n",
      "xlm_roberta                       1.013483       0.989353        0.963608   \n",
      "macbert                           1.066133       1.007996        1.007851   \n",
      "chinese_roberta                   1.003920       1.068012        1.005864   \n",
      "\n",
      "                 Scaled_Attention_Comp  Gradients_Comp  \\\n",
      "zhbert                        0.994799        0.999209   \n",
      "mbert                         0.995360        1.000964   \n",
      "xlm_roberta                   0.964288        1.015278   \n",
      "macbert                       1.007321        1.002373   \n",
      "chinese_roberta               1.005192        0.994387   \n",
      "\n",
      "                 Integrated_Gradients_Comp  Deeplift_Comp  Accuracy  \\\n",
      "zhbert                            0.999640       0.999115  0.835222   \n",
      "mbert                             1.005603       1.008984  0.836111   \n",
      "xlm_roberta                       1.006101       1.011642  0.726778   \n",
      "macbert                           0.997064       0.995906  0.846444   \n",
      "chinese_roberta                   0.996038       1.003061  0.851778   \n",
      "\n",
      "                           model dataset  \n",
      "zhbert                    zhbert     csl  \n",
      "mbert                      mbert     csl  \n",
      "xlm_roberta          xlm_roberta     csl  \n",
      "macbert                  macbert     csl  \n",
      "chinese_roberta  chinese_roberta     csl  \n",
      " \n",
      "evinf\n",
      "bert\n",
      "{'Attention_Suff': 1.5363029592752437, 'Scaled_Attention_Suff': 1.5485558557610177, 'Gradients_Suff': 1.094252755557506, 'Integrated_Gradients_Suff': 1.2081617718188984, 'Deeplift_Suff': 0.9886237483761672, 'Attention_Comp': 1.2677058005598023, 'Scaled_Attention_Comp': 1.267693591134925, 'Gradients_Comp': 1.0446644022145226, 'Integrated_Gradients_Comp': 1.1501872818912802, 'Deeplift_Comp': 1.043107766092951, 'F1': 0.6262390217157291}\n",
      "mbert\n",
      "{'Attention_Suff': 1.8698436032511812, 'Scaled_Attention_Suff': 1.8725815910972325, 'Gradients_Suff': 1.1655549362173971, 'Integrated_Gradients_Suff': 1.045743344002998, 'Deeplift_Suff': 0.9307992893672061, 'Attention_Comp': 1.1217165085250873, 'Scaled_Attention_Comp': 1.1217122225605953, 'Gradients_Comp': 1.0166406923578266, 'Integrated_Gradients_Comp': 1.0225111588427305, 'Deeplift_Comp': 1.0042316800592643, 'F1': 0.685839435269596}\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 0.9411514292149502, 'Scaled_Attention_Suff': 0.9597535011716012, 'Gradients_Suff': 1.0303340012787876, 'Integrated_Gradients_Suff': 0.9600314896058146, 'Deeplift_Suff': 0.9778939051509056, 'Attention_Comp': 0.9887899371816773, 'Scaled_Attention_Comp': 0.9837117253207918, 'Gradients_Comp': 1.0516512522639463, 'Integrated_Gradients_Comp': 1.164939124145968, 'Deeplift_Comp': 1.0045072919575777, 'F1': 0.338904639940293}\n",
      "roberta\n",
      "{'Attention_Suff': 1.22627518487548, 'Scaled_Attention_Suff': 1.219822168895569, 'Gradients_Suff': 1.1506328975105984, 'Integrated_Gradients_Suff': 1.120095429275962, 'Deeplift_Suff': 1.1234614218888728, 'Attention_Comp': 1.7891934022492364, 'Scaled_Attention_Comp': 1.791048874565436, 'Gradients_Comp': 1.2549057412772333, 'Integrated_Gradients_Comp': 1.4343582474701604, 'Deeplift_Comp': 1.5003914378267895, 'F1': 0.688159330250625}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "bert               1.536303               1.548556        1.094253   \n",
      "mbert              1.869844               1.872582        1.165555   \n",
      "xlm_roberta        0.941151               0.959754        1.030334   \n",
      "roberta            1.226275               1.219822        1.150633   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "bert                          1.208162       0.988624        1.267706   \n",
      "mbert                         1.045743       0.930799        1.121717   \n",
      "xlm_roberta                   0.960031       0.977894        0.988790   \n",
      "roberta                       1.120095       1.123461        1.789193   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "bert                      1.267694        1.044664                   1.150187   \n",
      "mbert                     1.121712        1.016641                   1.022511   \n",
      "xlm_roberta               0.983712        1.051651                   1.164939   \n",
      "roberta                   1.791049        1.254906                   1.434358   \n",
      "\n",
      "             Deeplift_Comp        F1        model dataset  \n",
      "bert              1.043108  0.626239         bert   evinf  \n",
      "mbert             1.004232  0.685839        mbert   evinf  \n",
      "xlm_roberta       1.004507  0.338905  xlm_roberta   evinf  \n",
      "roberta           1.500391  0.688159      roberta   evinf  \n",
      " \n",
      "sst\n",
      "xlm_roberta\n",
      "{'Attention_Suff': 1.0688094936580361, 'Scaled_Attention_Suff': 1.0714738494762963, 'Gradients_Suff': 0.9864963908475101, 'Integrated_Gradients_Suff': 1.1001114340634368, 'Deeplift_Suff': 0.9913638967805468, 'Attention_Comp': 0.9168108223446944, 'Scaled_Attention_Comp': 0.9191973407511004, 'Gradients_Comp': 0.9412759689220713, 'Integrated_Gradients_Comp': 1.1101456081938417, 'Deeplift_Comp': 0.9484586487451236, 'F1': 0.846416263359789}\n",
      "mbert\n",
      "{'Attention_Suff': 1.0776926120054768, 'Scaled_Attention_Suff': 1.0755506993833654, 'Gradients_Suff': 0.9989957402624257, 'Integrated_Gradients_Suff': 1.164830201056827, 'Deeplift_Suff': 1.1250927497319019, 'Attention_Comp': 0.917348016325238, 'Scaled_Attention_Comp': 0.9234691902248439, 'Gradients_Comp': 0.9141260138471738, 'Integrated_Gradients_Comp': 1.1557338604637417, 'Deeplift_Comp': 1.2408274316659658, 'F1': 0.8626781800297649}\n",
      "bert\n",
      "{'Attention_Suff': 1.0481223084511742, 'Scaled_Attention_Suff': 1.0494158610742312, 'Gradients_Suff': 1.0056845661286224, 'Integrated_Gradients_Suff': 1.0854813013267435, 'Deeplift_Suff': 1.0235982271842574, 'Attention_Comp': 1.162739992717646, 'Scaled_Attention_Comp': 1.1640828513299262, 'Gradients_Comp': 1.151312794651457, 'Integrated_Gradients_Comp': 1.5217616460207297, 'Deeplift_Comp': 1.0685208136588726, 'F1': 0.9155617816042192}\n",
      "roberta\n",
      "{'Attention_Suff': 1.1981218346698839, 'Scaled_Attention_Suff': 1.2038066639227005, 'Gradients_Suff': 0.9636503122815304, 'Integrated_Gradients_Suff': 1.0785716525493771, 'Deeplift_Suff': 1.0508302533620544, 'Attention_Comp': 1.257508414564188, 'Scaled_Attention_Comp': 1.263296317459447, 'Gradients_Comp': 1.0283530528014544, 'Integrated_Gradients_Comp': 1.1711634822306036, 'Deeplift_Comp': 1.0416901376915988, 'F1': 0.8893154480537969}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "xlm_roberta        1.068809               1.071474        0.986496   \n",
      "mbert              1.077693               1.075551        0.998996   \n",
      "bert               1.048122               1.049416        1.005685   \n",
      "roberta            1.198122               1.203807        0.963650   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "xlm_roberta                   1.100111       0.991364        0.916811   \n",
      "mbert                         1.164830       1.125093        0.917348   \n",
      "bert                          1.085481       1.023598        1.162740   \n",
      "roberta                       1.078572       1.050830        1.257508   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "xlm_roberta               0.919197        0.941276                   1.110146   \n",
      "mbert                     0.923469        0.914126                   1.155734   \n",
      "bert                      1.164083        1.151313                   1.521762   \n",
      "roberta                   1.263296        1.028353                   1.171163   \n",
      "\n",
      "             Deeplift_Comp        F1        model dataset  \n",
      "xlm_roberta       0.948459  0.846416  xlm_roberta     sst  \n",
      "mbert             1.240827  0.862678        mbert     sst  \n",
      "bert              1.068521  0.915562         bert     sst  \n",
      "roberta           1.041690  0.889315      roberta     sst  \n",
      " \n",
      "french_paws\n",
      "mbert\n",
      "{'Attention_Suff': 1.1142908445092485, 'Scaled_Attention_Suff': 1.1191234653029078, 'Gradients_Suff': 1.0011990734534026, 'Integrated_Gradients_Suff': 1.005245306971817, 'Deeplift_Suff': 0.926908496512529, 'Attention_Comp': 0.9915523419005873, 'Scaled_Attention_Comp': 0.9898698980403215, 'Gradients_Comp': 0.9985338502301863, 'Integrated_Gradients_Comp': 1.000198577379239, 'Deeplift_Comp': 1.004577466281791, 'Accuracy': 0.878833333333333}\n",
      "xlm-roberta\n",
      "{'Attention_Suff': 1.0361008520968067, 'Scaled_Attention_Suff': 1.0502024012677729, 'Gradients_Suff': 1.0241533685712039, 'Integrated_Gradients_Suff': 1.0206174645058883, 'Deeplift_Suff': 0.9997829216280032, 'Attention_Comp': 1.2133983394193149, 'Scaled_Attention_Comp': 1.2086067942203997, 'Gradients_Comp': 1.0606553541026913, 'Integrated_Gradients_Comp': 1.0005826676496283, 'Deeplift_Comp': 1.0218375212149517, 'Accuracy': 0.877833333333333}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "mbert              1.114291               1.119123        1.001199   \n",
      "xlm-roberta        1.036101               1.050202        1.024153   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "mbert                         1.005245       0.926908        0.991552   \n",
      "xlm-roberta                   1.020617       0.999783        1.213398   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "mbert                     0.989870        0.998534                   1.000199   \n",
      "xlm-roberta               1.208607        1.060655                   1.000583   \n",
      "\n",
      "             Deeplift_Comp  Accuracy        model      dataset  \n",
      "mbert             1.004577  0.878833        mbert  french_paws  \n",
      "xlm-roberta       1.021838  0.877833  xlm-roberta  french_paws  \n",
      " \n",
      "french_xnli\n",
      "xlm-roberta\n",
      "{'Attention_Suff': 0.9666027963852696, 'Scaled_Attention_Suff': 0.9731325682316564, 'Gradients_Suff': 1.0099600679429872, 'Integrated_Gradients_Suff': 0.9907547503130808, 'Deeplift_Suff': 1.0002822370132347, 'Attention_Comp': 0.9984618904515383, 'Scaled_Attention_Comp': 0.9964836448775162, 'Gradients_Comp': 1.0494531618182998, 'Integrated_Gradients_Comp': 1.0368672668887111, 'Deeplift_Comp': 0.9857827138184025, 'Accuracy': 0.788489687292082}\n",
      "mbert\n",
      "{'Attention_Suff': 0.9857421094038625, 'Scaled_Attention_Suff': 0.9868809003824195, 'Gradients_Suff': 1.0390549418147843, 'Integrated_Gradients_Suff': 1.0567788779439395, 'Deeplift_Suff': 0.9757940655361717, 'Attention_Comp': 1.0001944037189774, 'Scaled_Attention_Comp': 0.9999139367574179, 'Gradients_Comp': 0.9991890152567877, 'Integrated_Gradients_Comp': 0.9986982904848773, 'Deeplift_Comp': 1.0061108541370052, 'Accuracy': 0.774584165003326}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "xlm-roberta        0.966603               0.973133        1.009960   \n",
      "mbert              0.985742               0.986881        1.039055   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "xlm-roberta                   0.990755       1.000282        0.998462   \n",
      "mbert                         1.056779       0.975794        1.000194   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "xlm-roberta               0.996484        1.049453                   1.036867   \n",
      "mbert                     0.999914        0.999189                   0.998698   \n",
      "\n",
      "             Deeplift_Comp  Accuracy        model      dataset  \n",
      "xlm-roberta       0.985783  0.788490  xlm-roberta  french_xnli  \n",
      "mbert             1.006111  0.774584        mbert  french_xnli  \n",
      " \n",
      "french_csl\n",
      "xlm-roberta\n",
      "{'Attention_Suff': 1.1418396856833934, 'Scaled_Attention_Suff': 1.1264236116131028, 'Gradients_Suff': 0.9798245723459802, 'Integrated_Gradients_Suff': 0.9850697264200574, 'Deeplift_Suff': 0.995268230273873, 'Attention_Comp': 0.9288966179768461, 'Scaled_Attention_Comp': 0.9325160464878857, 'Gradients_Comp': 0.9922143368345435, 'Integrated_Gradients_Comp': 1.156042732615537, 'Deeplift_Comp': 0.9794362339857902, 'Accuracy': 0.8865000000000001}\n",
      "mbert\n",
      "{'Attention_Suff': 1.1959826586632931, 'Scaled_Attention_Suff': 1.1945832706332082, 'Gradients_Suff': 1.0010970891739404, 'Integrated_Gradients_Suff': 1.2573592114292087, 'Deeplift_Suff': 1.0497196347412514, 'Attention_Comp': 1.5480124634794252, 'Scaled_Attention_Comp': 1.557038986269123, 'Gradients_Comp': 1.0389397719303444, 'Integrated_Gradients_Comp': 1.6114770704902521, 'Deeplift_Comp': 1.1050306759694213, 'Accuracy': 0.877333333333333}\n",
      "french_roberta\n",
      "{'Attention_Suff': 1.2507235457908168, 'Scaled_Attention_Suff': 1.2498561207477406, 'Gradients_Suff': 1.1249392290101248, 'Integrated_Gradients_Suff': 1.4099082967457686, 'Deeplift_Suff': 1.3406279633835438, 'Attention_Comp': 1.4459484310885182, 'Scaled_Attention_Comp': 1.4470036673300528, 'Gradients_Comp': 1.2551155372725862, 'Integrated_Gradients_Comp': 1.828708766694379, 'Deeplift_Comp': 1.6441509230872842, 'Accuracy': 0.8668333333333331}\n",
      "                Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "xlm-roberta           1.141840               1.126424        0.979825   \n",
      "mbert                 1.195983               1.194583        1.001097   \n",
      "french_roberta        1.250724               1.249856        1.124939   \n",
      "\n",
      "                Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "xlm-roberta                      0.985070       0.995268        0.928897   \n",
      "mbert                            1.257359       1.049720        1.548012   \n",
      "french_roberta                   1.409908       1.340628        1.445948   \n",
      "\n",
      "                Scaled_Attention_Comp  Gradients_Comp  \\\n",
      "xlm-roberta                  0.932516        0.992214   \n",
      "mbert                        1.557039        1.038940   \n",
      "french_roberta               1.447004        1.255116   \n",
      "\n",
      "                Integrated_Gradients_Comp  Deeplift_Comp  Accuracy  \\\n",
      "xlm-roberta                      1.156043       0.979436  0.886500   \n",
      "mbert                            1.611477       1.105031  0.877333   \n",
      "french_roberta                   1.828709       1.644151  0.866833   \n",
      "\n",
      "                         model     dataset  \n",
      "xlm-roberta        xlm-roberta  french_csl  \n",
      "mbert                    mbert  french_csl  \n",
      "french_roberta  french_roberta  french_csl  \n",
      " \n",
      "spanish_csl\n",
      "xlm-roberta\n",
      "{'Attention_Suff': 1.041179645427736, 'Scaled_Attention_Suff': 1.0358438321562706, 'Gradients_Suff': 1.0020800393387224, 'Integrated_Gradients_Suff': 1.0614051847613029, 'Deeplift_Suff': 1.022391166077728, 'Attention_Comp': 0.9725964804258969, 'Scaled_Attention_Comp': 0.9752481873659127, 'Gradients_Comp': 0.9689076294799495, 'Integrated_Gradients_Comp': 1.161094354573411, 'Deeplift_Comp': 0.9822497201361066, 'Accuracy': 0.8781666666666661}\n",
      "mbert\n",
      "{'Attention_Suff': 1.2655165574960572, 'Scaled_Attention_Suff': 1.2729770962880436, 'Gradients_Suff': 1.0667818515847132, 'Integrated_Gradients_Suff': 1.3498448872332078, 'Deeplift_Suff': 1.0248488730871064, 'Attention_Comp': 1.5117517488916166, 'Scaled_Attention_Comp': 1.507700768992829, 'Gradients_Comp': 1.1266341823273882, 'Integrated_Gradients_Comp': 1.4407122105878098, 'Deeplift_Comp': 1.0986983997819144, 'Accuracy': 0.8861666666666661}\n",
      "BETO\n",
      "{'Attention_Suff': 0.9971179873913502, 'Scaled_Attention_Suff': 1.0086500787431303, 'Gradients_Suff': 0.9961387104526391, 'Integrated_Gradients_Suff': 1.0365910740400948, 'Deeplift_Suff': 1.0193880903454002, 'Attention_Comp': 0.9607889815151668, 'Scaled_Attention_Comp': 0.9556103259562939, 'Gradients_Comp': 1.0523634794173022, 'Integrated_Gradients_Comp': 1.3279012662123877, 'Deeplift_Comp': 1.025167528659228, 'Accuracy': 0.9063333333333331}\n",
      "             Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "xlm-roberta        1.041180               1.035844        1.002080   \n",
      "mbert              1.265517               1.272977        1.066782   \n",
      "BETO               0.997118               1.008650        0.996139   \n",
      "\n",
      "             Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "xlm-roberta                   1.061405       1.022391        0.972596   \n",
      "mbert                         1.349845       1.024849        1.511752   \n",
      "BETO                          1.036591       1.019388        0.960789   \n",
      "\n",
      "             Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "xlm-roberta               0.975248        0.968908                   1.161094   \n",
      "mbert                     1.507701        1.126634                   1.440712   \n",
      "BETO                      0.955610        1.052363                   1.327901   \n",
      "\n",
      "             Deeplift_Comp  Accuracy        model      dataset  \n",
      "xlm-roberta       0.982250  0.878167  xlm-roberta  spanish_csl  \n",
      "mbert             1.098698  0.886167        mbert  spanish_csl  \n",
      "BETO              1.025168  0.906333         BETO  spanish_csl  \n",
      " \n",
      "spanish_xnli\n",
      "mbert\n",
      "{'Attention_Suff': 0.9721876989362166, 'Scaled_Attention_Suff': 0.9770587118700036, 'Gradients_Suff': 1.069982476335528, 'Integrated_Gradients_Suff': 1.0311856882398809, 'Deeplift_Suff': 0.9642008440472006, 'Attention_Comp': 1.0012355532288708, 'Scaled_Attention_Comp': 1.0005201700165873, 'Gradients_Comp': 0.9981686310882673, 'Integrated_Gradients_Comp': 1.001133192652269, 'Deeplift_Comp': 1.0030662964559092, 'Accuracy': 0.787491683300066}\n",
      "BETO\n",
      "{'Attention_Suff': 1.0441732670786683, 'Scaled_Attention_Suff': 1.047190038143277, 'Gradients_Suff': 1.0095981921111017, 'Integrated_Gradients_Suff': 1.0126634176921343, 'Deeplift_Suff': 0.9817250698678295, 'Attention_Comp': 0.9968390349861259, 'Scaled_Attention_Comp': 0.9963040360415365, 'Gradients_Comp': 0.9985816429230893, 'Integrated_Gradients_Comp': 0.9979302225568915, 'Deeplift_Comp': 1.0005593259374839, 'Accuracy': 0.7842315369261471}\n",
      "       Attention_Suff  Scaled_Attention_Suff  Gradients_Suff  \\\n",
      "mbert        0.972188               0.977059        1.069982   \n",
      "BETO         1.044173               1.047190        1.009598   \n",
      "\n",
      "       Integrated_Gradients_Suff  Deeplift_Suff  Attention_Comp  \\\n",
      "mbert                   1.031186       0.964201        1.001236   \n",
      "BETO                    1.012663       0.981725        0.996839   \n",
      "\n",
      "       Scaled_Attention_Comp  Gradients_Comp  Integrated_Gradients_Comp  \\\n",
      "mbert               1.000520        0.998169                   1.001133   \n",
      "BETO                0.996304        0.998582                   0.997930   \n",
      "\n",
      "       Deeplift_Comp  Accuracy  model       dataset  \n",
      "mbert       1.003066  0.787492  mbert  spanish_xnli  \n",
      "BETO        1.000559  0.784232   BETO  spanish_xnli  \n"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-8yj67l0k/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
>>>>>>> 9b21b56250ec89375e33333d93853b8da641b433
     ]
    }
   ],
   "source": [
    "data_df_list = []\n",
    "\n",
    "for data in loaded_dict.keys():\n",
    "     df_list = []\n",
    "     print(' ')\n",
    "     print(data)\n",
    "     for model in loaded_dict[data].keys():\n",
    "          print(model)\n",
    "          print(loaded_dict[data][model])\n",
    "          df = pd.DataFrame.from_dict(loaded_dict[data][model], orient='index', columns=[model])\n",
    "          df = df.transpose()\n",
    "          df['model'] = model\n",
    "          df_list.append(df)\n",
    "     result = pd.concat(df_list)\n",
    "     result['dataset'] = data\n",
    "     # new_row = pd.Series(pd.Series(), index=result.columns)\n",
    "     # result = result.append(new_row)\n",
    "\n",
    "     print(result)\n",
    "     data_df_list.append(result)\n",
    "\n",
    "result = pd.concat(data_df_list)\n",
    "result.insert(0, 'model', result.pop('model'))\n",
    "result = result.set_index('dataset',drop=True).round(3)\n",
    "\n",
    "result.to_csv('summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xlm-roberta': {'Attention_Suff': 1.1418396856833934,\n",
       "  'Scaled_Attention_Suff': 1.1264236116131028,\n",
       "  'Gradients_Suff': 0.9798245723459802,\n",
       "  'Integrated_Gradients_Suff': 0.9850697264200574,\n",
       "  'Deeplift_Suff': 0.995268230273873,\n",
       "  'Attention_Comp': 0.9288966179768461,\n",
       "  'Scaled_Attention_Comp': 0.9325160464878857,\n",
       "  'Gradients_Comp': 0.9922143368345435,\n",
       "  'Integrated_Gradients_Comp': 1.156042732615537,\n",
       "  'Deeplift_Comp': 0.9794362339857902,\n",
       "  'Accuracy': 0.8865000000000001},\n",
       " 'mbert': {'Attention_Suff': 1.1959826586632931,\n",
       "  'Scaled_Attention_Suff': 1.1945832706332082,\n",
       "  'Gradients_Suff': 1.0010970891739404,\n",
       "  'Integrated_Gradients_Suff': 1.2573592114292087,\n",
       "  'Deeplift_Suff': 1.0497196347412514,\n",
       "  'Attention_Comp': 1.5480124634794252,\n",
       "  'Scaled_Attention_Comp': 1.557038986269123,\n",
       "  'Gradients_Comp': 1.0389397719303444,\n",
       "  'Integrated_Gradients_Comp': 1.6114770704902521,\n",
       "  'Deeplift_Comp': 1.1050306759694213,\n",
       "  'Accuracy': 0.877333333333333},\n",
       " 'french_roberta': {'Attention_Suff': 1.2507235457908168,\n",
       "  'Scaled_Attention_Suff': 1.2498561207477406,\n",
       "  'Gradients_Suff': 1.1249392290101248,\n",
       "  'Integrated_Gradients_Suff': 1.4099082967457686,\n",
       "  'Deeplift_Suff': 1.3406279633835438,\n",
       "  'Attention_Comp': 1.4459484310885182,\n",
       "  'Scaled_Attention_Comp': 1.4470036673300528,\n",
       "  'Gradients_Comp': 1.2551155372725862,\n",
       "  'Integrated_Gradients_Comp': 1.828708766694379,\n",
       "  'Deeplift_Comp': 1.6441509230872842,\n",
       "  'Accuracy': 0.8668333333333331}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict['french_csl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
